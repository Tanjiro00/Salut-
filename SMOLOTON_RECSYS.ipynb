{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U 'spacy[cuda11x,transformers,lookups]'\n",
        "!python -m spacy download ru_core_news_lg\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "FDu0ZeE6EE1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "KjngsS8JyAkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlbl9z7OBlpG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"ru_core_news_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_annotation_setter(docs, trf_data):\n",
        "    doc_data = list(trf_data.doc_data)\n",
        "    for doc, data in zip(docs, doc_data):\n",
        "        doc._.custom_attr = data\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "nlp.get_pipe(\"transformer\").set_extra_annotations = custom_annotation_setter\n",
        "doc = nlp(\"This is a text\")\n",
        "assert isinstance(doc._.custom_attr, TransformerData)\n",
        "print(doc._.custom_attr.tensors)"
      ],
      "metadata": {
        "id": "IKpY2MieuSg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "\n",
        "\n",
        "#Load AutoModel from huggingface model repository\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
        "model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
        "\n",
        "sentences = ['Привет! Как твои дела?',\n",
        "             'А правда, что 42 твое любимое число?']\n",
        "\n",
        "#Tokenize sentences\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
        "\n",
        "#Compute token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "#Perform pooling. In this case, mean pooling\n",
        "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n"
      ],
      "metadata": {
        "id": "Pb7VXYswtACY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output[0].shape"
      ],
      "metadata": {
        "id": "qryXKTLPzBtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _mean_pooling(self, model_output, attention_mask):\n",
        "  token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "  input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "  sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "  sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "  return sum_embeddings / sum_mask"
      ],
      "metadata": {
        "id": "gKJ98--C2g3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input['attention_mask']"
      ],
      "metadata": {
        "id": "D17M0MNF2g9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2core = {\n",
        "    'sbert': 'ai-forever/sbert_large_nlu_ru',\n",
        "    'spacy': 'en_core_web_sm'\n",
        "}"
      ],
      "metadata": {
        "id": "io9GhArWr4WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_vector(name_model, text):\n",
        "  if name_model == 'sbert':\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
        "    sbert_model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
        "  if name_model == 'spacy':\n",
        "    pass\n",
        ""
      ],
      "metadata": {
        "id": "JeKbG053GyTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Place:\n",
        "  def __init__(self,  tags, vec_model, text_tokenizer):\n",
        "    '''\n",
        "    tags = tags for each place or text prompt\n",
        "    vec_model = Language Model for getting tags embeddings\n",
        "    '''\n",
        "    self.vec_model = vec_model\n",
        "    self.text_tokenizer = text_tokenizer\n",
        "    self.tags = tags\n",
        "    self.embs = torch.tensor([])\n",
        "\n",
        "\n",
        "  def compute_embeddings(self, is_return=False):\n",
        "    tensor_tags = self.text_tokenizer(self.tags, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "      model_out = self.vec_model(**tensor_tags)\n",
        "    self.embs = self._mean_pooling(model_out).squeeze()\n",
        "    if is_return:\n",
        "      return self.embs\n",
        "\n",
        "\n",
        "\n",
        "  def _mean_pooling(self, model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask"
      ],
      "metadata": {
        "id": "Swn4BbhXE5Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cos_sim(self, idx, return_max=True):\n",
        "    '''\n",
        "    func get index of person and return cos_sim of person with every man from BD\n",
        "    '''\n",
        "    man = self.all_embeddings[idx]\n",
        "    indices = torch.tensor([i for i in range(self.all_embeddings.shape[0]) if i != idx])\n",
        "    other_embeddings = torch.index_select(self.all_embeddings, 0, indices)\n",
        "    cosine_arr = cos(man, other_embeddings)\n",
        "    if return_max:\n",
        "      max_sim_idx = torch.topk(cosine_arr, 5).indices\n",
        "      max_sim_person = indices[max_sim_idx]\n",
        "      return max_sim_person\n",
        "    min_sim_idx = torch.topk(1/(cosine_arr*100), 5).indices #I need get min topk but I lazy and I solve invert numbers(I dont want to search min topk method)\n",
        "    min_sim_person = indices[min_sim_idx]\n",
        "    return min_sim_person\n"
      ],
      "metadata": {
        "id": "TxusbhwCD_EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class User:\n",
        "  def __init__(self,  tags, prompt, vec_model, text_tokenizer, liked_history=None):\n",
        "    self.vec_model = vec_model\n",
        "    self.text_tokenizer = text_tokenizer\n",
        "    self.prompt = prompt\n",
        "    self.tags = tags\n",
        "    self.cos = torch.nn.CosineSimilarity(dim=0)\n",
        "    self.liked_history = liked_history\n",
        "    tensor_tags = self.text_tokenizer(tags, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
        "    tensor_prompt = self.text_tokenizer(prompt, padding=True, truncation=True, max_length=50, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "      model_out_tags = self.vec_model(**tensor_tags)\n",
        "      model_out_prompt = self.vec_model(**tensor_prompt)\n",
        "    embs_tags = self._mean_pooling(model_out_tags).squeeze()\n",
        "    embs_prompt = self._mean_pooling(model_out_prompt).squeeze()\n",
        "    self.user_embs = (embs_tags + embs_prompt) / 2\n",
        "\n",
        "\n",
        "  def get_topk_rec(self, idx, place_embeddings, k=5):\n",
        "    '''\n",
        "    func get index of person and return cos_sim of person with every man from BD\n",
        "    '''\n",
        "    man = self.all_embeddings[idx]\n",
        "    indices = torch.tensor([i for i in range(self.place_embeddings.shape[0])])\n",
        "    other_embeddings = torch.index_select(self.all_embeddings, 0, indices)\n",
        "    cosine_arr = self.cos(man, place_embeddings)\n",
        "    max_sim_idx = torch.topk(cosine_arr, k).indices\n",
        "    max_sim_person = indices[max_sim_idx]\n",
        "    return max_sim_person\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def _mean_pooling(self, model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask"
      ],
      "metadata": {
        "id": "mLcSv0br32_q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}